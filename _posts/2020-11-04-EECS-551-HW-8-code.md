---
title: EECS 551 HW 8 code
date: 2020-11-04 03:11:21 +0800
categories: [Learning, UMICH, EECS551]
tags: [EECS551, UMICH, Data Science, Linear Algebra, DSP, ML]
---
## compress_image
```julia
using LinearAlgebra
"""
Ac, r = compress_image(A, p)
In:
* `A` `m × n` matrix
* `p` scalar in `(0, 1]`
Out:
* `Ac` a `m × n` matrix containing a compressed version of `A`
that can be represented using at most `(100 * p)%` as many bits
required to represent `A`
* `r` the rank of `Ac`
"""
function compress_image(A, p)
    m,n = size(A)
    U,s,V = svd(A);
    r = floor(Int,p*m*n/(m+n+1));
    Ac = U[:,1:r]*Diagonal(s[1:r])*V[:,1:r]';
    return Ac,r
end
```
## optshrink1
version 1_test
```julia
using LinearAlgebra
"""
Xh = optshrink1(Y::AbstractMatrix, r::Int)
Perform rank−r denoising of data matrix `Y` using the OptShrink method
by Prof. Nadakuditi in this May 2014 IEEE Tr. on Info. Theory paper:
http://doi.org/10.1109/TIT.2014.2311661
In:
− `Y` 2D array where `Y = X + noise` and goal is to estimate `X`
− `r` estimated rank of `X`
Out:
− `Xh` rank−`r` estimate of `X` using OptShrink weights for SVD components
This version works only if the size of `Y` is sufficiently small,
because it performs calculations involving arrays roughly of
`size(Y'*Y)` and `size(Y*Y')`, so neither dimension of `Y` can be large.
"""
function optshrink1(Y::AbstractMatrix, r::Int)
    U,s,V = svd(Y,full = true);
    m,n = size(Y);
    X = make_rectangular_diag(s,m,n)[r+1:end,r+1:end];
    z = s[1:r]
    w = zeros(r);
    for i = 1:r
        w[i] = -2*D_hat(X,z[i])/D_hat_t(X,z[i]);
    end
    return U[:,1:r]*Diagonal(w)*V[:,1:r]'
end
function make_rectangular_diag(s,m,n)
    Sigma = zeros(m,n);
    for i = 1:min(m,n)
        Sigma[i,i] = s[i];
    end
    return Sigma
end
function D_hat(X,z)
    z2IXXt = z^2*I - X*X';
    z2IXtX = z^2*I - X'*X;
    invz2XtX = z2IXtX^(-1);
    invz2XXt = z2IXXt^(-1);
    D1z = 1/n*tr(z*invz2XXt);
    D2z = 1/m*tr(z*invz2XtX);
    return D1z*D2z;
end

function D_hat_t(X,z)
    z2IXXt = z^2*I - X*X';
    z2IXtX = z^2*I - X'*X;
    invz2XtX = z2IXtX^(-1);
    invz2XXt = z2IXXt^(-1);
    D1z = 1/n*tr(z*invz2XXt);
    D2z = 1/m*tr(z*invz2XtX);
    D1zp = 1/n*tr(-2*z^2*invz2XXt^2+invz2XXt);
    D2zp = 1/m*tr(-2*z^2*invz2XtX^2+invz2XtX);
    return D1z*D2zp+D1zp*D2z;
end
```
## optshrink1
version 2
```julia
using LinearAlgebra
"""
Xh = optshrink1(Y::AbstractMatrix, r::Int)
Perform rank−r denoising of data matrix `Y` using the OptShrink method
by Prof. Nadakuditi in this May 2014 IEEE Tr. on Info. Theory paper:
http://doi.org/10.1109/TIT.2014.2311661
In:
− `Y` 2D array where `Y = X + noise` and goal is to estimate `X`
− `r` estimated rank of `X`
Out:
− `Xh` rank−`r` estimate of `X` using OptShrink weights for SVD components
This version works only if the size of `Y` is sufficiently small,
because it performs calculations involving arrays roughly of
`size(Y'*Y)` and `size(Y*Y')`, so neither dimension of `Y` can be large.
"""
function optshrink1(Y::AbstractMatrix, r::Int)
    U,s,V = svd(Y);
    m,n = size(Y);
    Sigma_r = zeros(m,n);
    for i = 1:min(m,n)
        Sigma_r[i,i] = s[i];
    end
    Sigma_r = Sigma_r[r+1:end,r+1:end];
    @show n,m = size(Sigma_r);
    w = zeros(r);
    for i = 1:r
        D_hat = tr(s[i]*(s[i]^2*I-Sigma_r*Sigma_r')^(-1))*tr(s[i]*(s[i]^2*I-Sigma_r'*Sigma_r)^(-1))/(m*n);
        part_1 = tr(s[i]*(s[i]^2*I-Sigma_r*Sigma_r')^(-1));
        @show part_1
        part_2 = tr(-2*s[i]^2*(s[i]^2*I-Sigma_r'*Sigma_r)^(-2)+(s[i]^2*I-Sigma_r'*Sigma_r)^(-1));
        @show part_2
        part_3 = part_1*part_2/(m*n);
        @show part_3
        part_4 = tr(s[i]*(s[i]^2*I-Sigma_r'*Sigma_r)^(-1));
        @show part_4
        part_5 = tr(-2*s[i]^2*(s[i]^2*I-Sigma_r*Sigma_r')^(-2)+(s[i]^2*I-Sigma_r*Sigma_r')^(-1));
        @show part_5
        test1 = tr(-2*s[i]^2*(s[i]^2*I-Sigma_r*Sigma_r')^(-2));
        @show test1
        test2 = tr((s[i]^2*I-Sigma_r*Sigma_r')^(-1));
        @show test2
        @show test1+test2
        test4 = tr((s[i]^2*I-Sigma_r*Sigma_r')^(-2));
        @show test4
        test5 = tr((s[i]^2*I-Sigma_r*Sigma_r')^(-1))^2
        @show test5
        @show global test1 = (s[i]^2*I-Sigma_r*Sigma_r')^(-1);
        @show global test2 = (s[i]^2*I-Sigma_r*Sigma_r')^(-2);
        part_6 = part_4*part_5/(m*n);
        @show part_6
        part_7 = part_3+part_6
        @show part_7
        #D_hat_t = tr(s[i]*(s[i]^2*I-Sigma_r*Sigma_r')^(-1))*tr(-2*s[i]^2*(s[i]^2*I-Sigma_r'*Sigma_r)^(-2)+(s[i]^2*I-Sigma_r'*Sigma_r)^(-1))/(m*n)
        #@show D_hat_t
        #second_part = tr(s[i]*(s[i]^2*I-Sigma_r'*Sigma_r)^(-1))*tr(-2*s[i]^2*(s[i]^2*I-Sigma_r*Sigma_r')^(-2)+(s[i]^2*I-Sigma_r*Sigma_r')^(-1))/(m*n);
        #@show second_part
        D_hat_t = part_7
        @show D_hat
        #@show D_hat_t
        @show w[i] = -2*D_hat/D_hat_t;
    end
    return U[:,1:r]*Diagonal(w)*V[:,1:r]'
end
# include("optshrink1.jl") # uncomment if you need this
using Random: seed!
using LinearAlgebra: norm
seed!(0)
X = randn(30) * randn(20)' # outer product
Y = X + 40 * randn(size(X))
Xh_opt = optshrink1(Y, 1)
U,s,V = svd(Y)
Xh_lr = U[:,1]*s[1]*V[:,1]'; # you finish this to make the conventional rank−1 approximation
@show norm(Xh_opt - X)
@show norm(Xh_lr - X)
```
## optshrink2
version 1_test
```julia
using LinearAlgebra
"""
Xh = optshrink2(Y::AbstractMatrix, r::Int)
Perform rank−`r` denoising of data matrix `Y` using the OptShrink method
by Prof. Nadakuditi in this May 2014 IEEE Tr. on Info. Theory paper:
http://doi.org/10.1109/TIT.2014.2311661
In:
− `Y` 2D array where `Y = X + noise` and goal is to estimate `X`
− `r` estimated rank of `X`
Out:
− `Xh` rank−`r` estimate of `X` using OptShrink weights for SVD components
This version works even if one of the dimensions of `Y` is large,
as long as the other is sufficiently small.
"""
function optshrink2(Y::AbstractMatrix, r::Int)
    U,s,V = svd(Y);
    m,n = size(Y)
    w = zeros(r);
    @show m = m-r;
    @show n = n-r;
    for i = 1:r
        sum_tr_l = sum(mapslices((x)->1/(s[i]^2-x[1]^2),s[r+1:end]',dims = 1));
        sum_tr_l_2 = sum(mapslices((x)->1/(s[i]^2-x[1]^2)^2,s[r+1:end]',dims = 1));
        sum_tr_m = sum_tr_l + abs(m-n)/s[i]^2;
        sum_tr_m_2 = sum_tr_l_2 + abs(m-n)/s[i]^4;
        D_hat = s[i]^2*sum_tr_l*sum_tr_m/(m*n);
        D_hat_t = (sum_tr_m*(-2*s[i]^2*sum_tr_l_2+sum_tr_l)+sum_tr_l*(-2*s[i]^2*sum_tr_m_2+sum_tr_m))*s[i]/(m*n);
        part_1 = s[i]*sum_tr_m;
        @show part_1
        part_2 = -2*s[i]^2*sum_tr_l_2+sum_tr_l;
        @show part_2
        test1 = -2*s[i]^2*sum_tr_l_2
        @show test1
        test2 = sum_tr_l;
        @show test2
        @show test1+test2
        part_3 = part_1*part_2/(m*n);
        @show part_3
        part_4 = s[i]*sum_tr_l;
        @show part_4
        part_5 = -2*s[i]^2*sum_tr_m_2+sum_tr_m;
        @show part_5
        part_6 = part_4*part_5/(m*n);
        @show part_6
        part_7 = part_3+part_6;
        @show part_7
        #part_1 = s[i]*sum_tr_m*sum_tr_l/(m*n)*(1-2*s[i]^2*sum_tr_m);
        #@show part_1;
        #part_2 = s[i]*sum_tr_m*sum_tr_l/(m*n)*(1-2*s[i]^2*sum_tr_l);
        #@show part_2;
        @show D_hat
        #@show D_hat_t
        @show w[i] = -2*D_hat/D_hat_t;
    end
    return U[:,1:r]*Diagonal(w)*V[:,1:r]'
end
using Random: seed!
using LinearAlgebra: norm
seed!(0)
X = randn(30) * randn(20)' # outer product
Y = X + 40 * randn(size(X))
Xh_opt = optshrink1(Y, 1)
Xh_lr = optshrink2(Y, 1)
@show norm(Xh_opt - X)
@show norm(Xh_lr - X)
```
## optshrink2
version 2
```julia
using LinearAlgebra
"""
Xh = optshrink2(Y::AbstractMatrix, r::Int)
Perform rank−`r` denoising of data matrix `Y` using the OptShrink method
by Prof. Nadakuditi in this May 2014 IEEE Tr. on Info. Theory paper:
http://doi.org/10.1109/TIT.2014.2311661
In:
− `Y` 2D array where `Y = X + noise` and goal is to estimate `X`
− `r` estimated rank of `X`
Out:
− `Xh` rank−`r` estimate of `X` using OptShrink weights for SVD components
This version works even if one of the dimensions of `Y` is large,
as long as the other is sufficiently small.
"""
function optshrink2(Y::AbstractMatrix, r::Int)
    U,s,V = svd(Y);
    m,n = size(Y)
    w = zeros(r);
    m = m-r;
    n = n-r;
    for i = 1:r
        sum_tr_l = sum(mapslices((x)->1/(s[i]^2-x[1]^2),s[r+1:end]',dims = 1));
        sum_tr_l_2 = sum(mapslices((x)->1/(s[i]^2-x[1]^2)^2,s[r+1:end]',dims = 1));
        sum_tr_m = sum_tr_l + abs(m-n)/s[i]^2;
        sum_tr_m_2 = sum_tr_l_2 + abs(m-n)/s[i]^4;
        D_hat = s[i]^2*sum_tr_l*sum_tr_m/(m*n);
        D_hat_t = (sum_tr_m*(-2*s[i]^2*sum_tr_l_2+sum_tr_l)+sum_tr_l*(-2*s[i]^2*sum_tr_m_2+sum_tr_m))*s[i]/(m*n);
        w[i] = -2*D_hat/D_hat_t;
    end
    return U[:,1:r]*Diagonal(w)*V[:,1:r]'
end

# include("optshrink2.jl") # uncomment if needed
using LinearAlgebra: norm
using Random: seed!
seed!(0)
X = randn(10^5) * randn(100)' / 8 # test large case now
Y = X + randn(size(X))
Xh_opt = optshrink2(Y, 1)
U,s,V = svd(Y)
Xh_lr = U[:,1]*s[1]*V[:,1]';
@show norm(Xh_opt - X)
@show norm(Xh_lr - X)
```
## shrink_p_1_2
```julia
"""
out = shrink_p_1_2(v, reg::Real)
Compute minimizer of ``1/2 |v − x|^2 + reg |x|^p``
for `p=1/2` when `v` is real and nonnegative.
In:
* `v` scalar, vector, or array of (real, nonnegative) input values
* `reg` regularization parameter
Out:
* `xh` solution to minimization problem for each element of `v`
(same size as `v`)
"""
function shrink_p_1_2(v, reg::Real)
    m = 0;
    n = 0;
    try
        m,n = size(v);
    catch e
        n = 1
        m = size(v)[1];
    end
    x_h = zeros(m,n);
    bound = 1.5*reg^(2/3);
    for j = 1:n
        for i = 1:m
            if v[i,j] > bound
                x_h[i,j] = 4/3*v[i,j]*cos(1/3*acos(-(3^(3/2)*reg)/(4*v[i,j]^(3/2))))^2;
            else
                x_h[i,j] = 0;
            end
        end
    end
    return x_h
end
reg = 2
#Define singular value soft thresholding (SVST)
SVST = (X,beta) -> begin
    sthresh = max.(v .- beta,0)
    return sthresh
end;
using Plots
v = LinRange(0, 8*reg, 801);

res = shrink_p_1_2(v,reg);
soft_res = SVST(v,reg);
plot(v,res,label="shrinkage function")
plot!(v,soft_res,label = "soft thresholding")

function test(x,reg)
    return 4/3*x*cos(1/3*acos(-(3^(3/2)*reg)/(4*x^(3/2))))^2
end
function test1(x,reg)
    return -(3^(3/2)*reg)/(4*x^(3/2))
end

x = -1.0:0.001:1.0
#plot(v,test1.(v,2))
```
## dist2locs
```julia
using LinearAlgebra
using Statistics
# make sure to include any using statements before submitting to the autograder

"""
Xr = dist2locs(D, d)

In:
* `D` is an `n x n` matrix such that `D[i, j]` is the distance from object `i` to object `j`
* `d` is the desired embedding dimension.

Out:
* `Xr` is an `d x n` matrix whose columns contains the relative coordinates of the `n` objects

Note: MDS is only unique up to rotation and translation,
so we enforce the following conventions on Xr in this order:
* [ORDER] `Xr[i,:]` corresponds to ith largest eigenpair of `C' * C`
* [CENTER] The centroid of the coordinates is zero
* [SIGN] The largest magnitude element of `Xr[i,:]` is positive
"""
function dist2locs(D, d)
    S = D.^2
    S = 0.5 * (S + S')
    G = S .- mean(S,dims=1)
    G = G .- mean(G,dims=2)
    G = -1/2 * G
    (_, s, V) = svd(G)
    Xr = Diagonal(sqrt.(s[1:d])) * V[:,1:d]'
    for i = 1:d
        if real(Xr[findmax(norm.(Xr[i,:]))[2]]) < 0
            Xr[1,:] = -Xr[1,:]
        end
    end
    return Xr
end
```